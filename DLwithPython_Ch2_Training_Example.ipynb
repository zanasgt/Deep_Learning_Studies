{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLwithPython_Ch2_Training_Example.ipynb adlı not defterinin kopyası",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy8+PfIMCtTS7I62LC1PZq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NTSvOGIH6MQn",
        "outputId": "d106a2e0-ddf4-4da9-ee8a-9f0681e42983"
      },
      "source": [
        "# MANUPILATING TENSOR DATA in NumPy\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(train_images, train_label), (test_images, test_label)=mnist.load_data()\n",
        "\n",
        "'''#Slicing the image data to take the interested area as we take 90 images below\n",
        "my_slice=train_images[10:100]\n",
        "print(my_slice.shape)\n",
        "\n",
        "#Also we can take part of the inner matrix\n",
        "my_slice=train_images[10:100, 0:24, 0:24]\n",
        "print(my_slice.shape)\n",
        "#we can also determine the boundary from the last and first elements in the demanded range\n",
        "my_slice = train_images[:, 7:-7, 7:-7]\n",
        "print(my_slice.shape)'''\n",
        "\n",
        "# we load our image data now we should re arrange input for process\n",
        "# these inputs store as numpy array in type float32\n",
        "train_images.shape\n",
        "train_images = train_images.reshape(60000, 28*28)\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape(10000, 28*28)\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "example_image = test_images[19]\n",
        "print(example_image.shape, example_image.ndim, type(example_image))\n",
        "\n",
        "# Already in numpy array format with dtype = 'float32', \n",
        "# if were not\n",
        "# example_image = np.array(example_image, dtype='float32')\n",
        "\n",
        "# We need to reshape as it include 2 layer (3rd is gray)\n",
        "example_image = example_image.reshape((28,28))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(example_image, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784,) 1 <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2921f80750>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we can indicate our model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "                           layers.Dense(512, activation='relu'),\n",
        "                           layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "#now we can arrange optimization and loss function\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics='accuracy')\n",
        "\n",
        "#finally training loops\n",
        "model.fit(train_images, train_label, batch_size=256, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFfifB0KYNIe",
        "outputId": "bdca43e9-d129-4ac7-e293-c890c9740f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - 3s 10ms/step - loss: 0.3160 - accuracy: 0.9093\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1308 - accuracy: 0.9624\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0865 - accuracy: 0.9747\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0624 - accuracy: 0.9818\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0477 - accuracy: 0.9859\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f29261cced0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh6QDhB69xe1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}